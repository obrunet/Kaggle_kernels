{"cells":[{"metadata":{},"cell_type":"markdown","source":"# Dogs and Cats\n\n<img src='https://images.pexels.com/photos/1909802/pexels-photo-1909802.jpeg?auto=compress&cs=tinysrgb&dpr=3&h=750&w=1260' style=\"height: 400px;\">\n\nPhoto by [Sharon McCutcheon](https://www.pexels.com/@mccutcheon)\n\n## Context\n\n* Type : Computer Vision (use of CNN)\n* Dataset : refer to [this Kaggle's competition](https://www.kaggle.com/c/dogs-vs-cats)\n\n## Goal \nClassify whether images contain either a dog or a cat. \n\n---"},{"metadata":{"ExecuteTime":{"end_time":"2019-06-05T12:39:38.913386Z","start_time":"2019-06-05T12:39:38.201249Z"},"trusted":true},"cell_type":"code","source":"import os, re, cv2, random\nimport numpy as np\nimport pandas as pd                       # only for the creation of a csv submission file\nimport matplotlib.pyplot as plt\nimport seaborn as sns","execution_count":null,"outputs":[]},{"metadata":{"ExecuteTime":{"end_time":"2019-06-05T12:39:38.198312Z","start_time":"2019-06-05T12:39:34.140224Z"},"trusted":true},"cell_type":"code","source":"from tensorflow.keras.layers import MaxPooling2D, Conv2D, Flatten, Dense\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n\nfrom sklearn.model_selection import train_test_split","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Preparation"},{"metadata":{},"cell_type":"markdown","source":"First, we've to retrieve sorted lists of all image files in both the train & test directories"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dir = '../input/train/'\ntest_dir = '../input/test/'\n\n# list of all img files for both train & test dataset\ntrain_img = [train_dir + i for i in os.listdir(train_dir)]\ntest_img  = [test_dir  + i for i in os.listdir(test_dir)]\n\n\n# function to sort the image files based on the numeric value in each file name, credits : sarvajna\n\ndef atoi(text):\n    return int(text) if text.isdigit() else text\n\ndef natural_keys(text):\n    return [atoi(c) for c in re.split('(\\d+)', text)]\n\n# sort the lists\ntrain_img.sort(key=natural_keys)\ntest_img.sort(key=natural_keys)\n#train_img","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The train folder contains 25,000 images of dogs and cats. Each image in this folder has the label as part of the filename. The test folder contains 12,500 images, named according to a numeric id. For each image in the test set, you should predict a probability that the image is a dog (1 = dog, 0 = cat)."},{"metadata":{"trusted":true},"cell_type":"code","source":"print('train images nb:', len(train_img))\nprint('test images nb:', len(test_img))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look at the first and last images, they are in color (so channel = 3 unlike black/white ones where channel = 1) and the size seems to be 150 x 150 px. Visualization of other few images choosen randomly :"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_width, img_height, channels = 150, 150, 3\n\ndef read_images(one_img):\n    img = cv2.imread(one_img, cv2.IMREAD_ANYCOLOR)\n    img_arr = cv2.resize(img,(img_width,img_height), interpolation=cv2.INTER_CUBIC)    # resize if needed\n    img_arr = img_arr / 255.0 # scaling\n    return img_arr\n\n# display 5 randomly choosen images\nplt.figure(figsize=(12, 5))\n\nfor i in range(1, 6):\n    plt.subplot(1, 5, i)\n    num = random.randint(0, len(train_img))\n    plt.imshow(read_images(train_img[num]))\n    #plt.title(class_names[y_train[num]])\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's use the openCV library to represent images in numbers, i.e read and resize the images.\nWe are in a case supervised learning classification, so we also have to generate labels for the train set.\n"},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = [], []   # creation of 2 arrays, X for img, and y for labels\n    \nfor img in train_img:\n    if 'cat' in img:\n        y.append(0)\n    elif 'dog' in img:\n        y.append(1)\n    X.append(cv2.resize(cv2.imread(img), (img_width, img_height), interpolation=cv2.INTER_CUBIC))\n\nX, y = np.array(X), np.array(y)\nX.shape, y.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.countplot(y)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The two classes seems to have the same nb of images for both categories and therefore are balanced."},{"metadata":{},"cell_type":"markdown","source":"Last, we've to split the train data set into 2 parts : a training set & a validation one. They will be used later to compute the accuracy and loss on the validation set while fitting the model using training set."},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\nX_train.shape, X_val.shape, y_train.shape, y_val.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"nb_train_samples = len(X_train)\nnb_validation_samples = len(X_val)\nbatch_size = 32","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"----\n\n# Model Evaluation\n\nWe're going to build a Convolutional Neural Network (CNN).\n\n## Theory\nSummary from this source : [towardsdatascience@_sumitsaha_](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53) \n<img src='https://cdn-images-1.medium.com/max/1800/1*vkQ0hXDaQv57sALXAJquxA.jpeg' style=\"height: 200px;\">\nA Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various aspects/objects in the image and be able to differentiate one from the other. The pre-processing required in a ConvNet is much lower as compared to other classification algorithms. While in primitive methods filters are hand-engineered, with enough training, ConvNets have the ability to learn these filters/characteristics.\n\nA ConvNet is able __to successfully capture the Spatial and Temporal dependencies in an image__ through the application of relevant filters. The architecture performs a better fitting to the image dataset due to the reduction in the number of parameters involved and reusability of weights. In other words, the network can be trained to understand the sophistication of the image better.\n\nA CNN is sequence of convolution layers pooling layers between, finished by a fully connected layer.\n\n<img src='https://cdn-images-1.medium.com/max/900/1*15yDvGKV47a0nkf5qLKOOQ.png' style=\"height: 200px;\">\n The role of the ConvNet is to reduce the RGB images into a form which is easier to process, without losing features which are critical for getting a good prediction.\n \n__Convolution Layer — The Kernel__\n <img src='https://cdn-images-1.medium.com/max/1200/1*ciDgQEjViWLnCbmX-EeSrA.gif' style=\"height: 200px;\">\n The filter or kernel moves to the right with a certain Stride Value till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same Stride Value and repeats the process until the entire image is traversed.\n\n<img src='https://cdn-images-1.medium.com/max/900/1*nYf_cUIHFEWU1JXGwnz-Ig.gif' style=\"height: 200px;\">\n\n The objective of the Convolution Operation is to extract the high-level features such as edges, from the input image. ConvNets need not be limited to only one Convolutional Layer. Conventionally, the first ConvLayer is responsible for capturing the Low-Level features such as edges, color, gradient orientation, etc. With added layers, the architecture adapts to the High-Level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would.\n \n __Pooling Layer__\n \n <img src='https://cdn-images-1.medium.com/max/900/1*nYf_cUIHFEWU1JXGwnz-Ig.gif' style=\"height: 200px;\">\n \n Similar to the Convolutional Layer, the Pooling layer is responsible for reducing the spatial size of the Convolved Feature. This is to decrease the computational power required to process the data through dimensionality reduction. Furthermore, it is useful for extracting dominant features which are rotational and positional invariant, thus maintaining the process of effectively training of the model.\n \n  <img src='https://cdn-images-1.medium.com/max/900/1*uoWYsCV5vBU8SHFPAPao-w.gif' style=\"height: 200px;\">\n  \n  __Classification — Fully Connected Layer__\n  \n Adding a Fully-Connected layer is a (usually) cheap way of learning non-linear combinations of the high-level features as represented by the output of the convolutional layer. The Fully-Connected layer is learning a possibly non-linear function in that space.\n\n  <img src='https://sds-platform-private.s3-us-east-2.amazonaws.com/uploads/35_blog_image_27.png' style=\"height: 200px;\">\n\nNow that we have converted our input image into a suitable form for our Multi-Level Perceptron, we shall flatten the image into a column vector. The flattened output is fed to a feed-forward neural network and backpropagation applied to every iteration of training. Over a series of epochs, the model is able to distinguish between dominating and certain low-level features in images and classify them using the Softmax Classification technique in case of mutiple class. Here we'll use the sigmoid activation function for a binray classification task."},{"metadata":{},"cell_type":"markdown","source":"## Model architecture"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_model():\n    \n    model = Sequential()\n\n    # Layer C1\n    model.add(Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(img_width, img_height, channels)))\n    # Layer S2\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    # Layer C3\n    model.add(Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\n    # Layer S4\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    # Before going into layer C5, we flatten our units\n    model.add(Flatten())\n    # Layer C5\n    model.add(Dense(units=120, activation='relu'))\n    # Layer F6\n    model.add(Dense(units=84, activation='relu'))\n    # Output layer\n    model.add(Dense(units=1, activation = 'sigmoid'))\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy']) # rmsprop ?\n\n# Define the callbacks\ncallbacks = [EarlyStopping(monitor='val_loss', patience=5),\n            TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"> ## Model training\n\nSince data is huge and might not fit in your computer memory, use keras `ImageDataGenerator` and `fit_generator` to train your model on the data. This is a case where the `fit_generator` becomes really useful."},{"metadata":{"trusted":true},"cell_type":"code","source":"# augmentation configuration used for training and validation\n\ntrain_datagen = ImageDataGenerator(\n    rescale = 1./255.0,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True)\n\nval_datagen = ImageDataGenerator(\n    rescale = 1./255.0,\n    shear_range = 0.2,\n    zoom_range = 0.2,\n    horizontal_flip = True)\n\n# prepare of generators for training and validation sets\n\ntrain_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\nvalidation_generator = val_datagen.flow(X_val, y_val, batch_size=batch_size)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit_generator(\n        train_generator,\n        steps_per_epoch=nb_train_samples // batch_size,\n        epochs=15,\n        validation_data=validation_generator,\n        validation_steps=nb_validation_samples // batch_size,\n        callbacks=callbacks)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# saving model and it's weights\nmodel.save_weights('model_weights.h5')\nmodel.save('model_keras.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Performances estimation\n\nAfter several tries, i've changed a little bit the architecture of the model (the number of layers and/or the number of units), the hyperparameters, and used data augmentation in order to improve the results. When it comes to estimate this model's performances, we can see that:\n* the loss has decreased from 0.58 to 0.31\n* the accuracy has increased from 0.68 to 0.86, while the accuracy on the validation dataset has changed from 0.72 to 0.81. This mean that the model has learnt and is able to generalize on another dataset."},{"metadata":{"trusted":true},"cell_type":"code","source":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nplt.figure()\nepochs = range(1,len(acc)+1)\nplt.plot(epochs,acc,'bo',label = 'Training acc')\nplt.plot(epochs,val_acc,'b',label = 'Validation acc')\nplt.title('Training and Validation accuracy')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.legend()\nplt.show()\n\nplt.figure()\nepochs = range(1,len(acc)+1)\nplt.plot(epochs,loss,'bo',label = 'Training loss')\nplt.plot(epochs,val_loss,'b',label = 'Validation loss')\nplt.title('Training and Validation loss')\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.legend()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = [] \n    \nfor img in test_img:\n    X_test.append(cv2.resize(cv2.imread(img), (img_width, img_height), interpolation=cv2.INTER_CUBIC))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test = np.array(X_test)\ny_test = np.zeros(X_test.shape[0])\nX_test.shape, y_test.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# augmentation configuration we will use for testing. Only rescaling.\ntest_datagen = ImageDataGenerator(rescale=1. / 255)\n\n# preparation of the generator for the test dataset\ntest_generator = val_datagen.flow(X_test, batch_size=batch_size)\n\n# create a df with predictions probabilities\npredictions_proba = model.predict_generator(test_generator, verbose=1)\ndf_submission = pd.DataFrame({'id': range(1, len(test_images_dogs_cats) + 1), 'label': list(predictions_proba)}) ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# change the label\ndf_submission['label'] = df_submission['label'].str.lstrip('[').str.rstrip(']')\ndf_submission['label'] = df_submission['label'].astype(float)\n\n# generate a csv file\ndf_submission.to_csv(\"dogs-vs-cats.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# display 5 randomly choosen images with predictions\nplt.figure(figsize=(12, 5))\n\nfor i in range(1, 6):\n    plt.subplot(1, 5, i)\n    num = random.randint(0, len(test_img))\n    plt.imshow(read_images(test_img[num]))\n    \n    pred = df_submission.iloc[num, 1]\n    if predictions[i, 0] >= 0.5: \n        plt.title('{:.2%} sure this is a Dog'.format(pred))\n        print()\n    else:\n        plt.title('{:.2%} sure this is a Cat'.format(1-pred))\n\nplt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":false,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":1}